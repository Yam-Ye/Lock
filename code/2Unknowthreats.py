import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
import os

# è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei'] 
plt.rcParams['axes.unicode_minus'] = False

# ================= 1. æ™ºèƒ½é…ç½®ä¸è·¯å¾„è‡ªæ£€ =================
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# å‘ä¸Šä¸¤çº§å¯»æ‰¾ Dataset (æ ¹æ®æ‚¨çš„ç›®å½•ç»“æ„è°ƒæ•´)
PATH_FEAT = os.path.join(CURRENT_DIR, "../Dataset/Mirai_dataset.csv")
PATH_LBL = os.path.join(CURRENT_DIR, "../Dataset/Mirai_labels.csv")

# å†²åˆºå‚æ•°
BATCH_SIZE = 2048       
EPOCHS = 100            
LATENT_DIM = 128        
NUM_PROTOTYPES = 200    
LR = 0.0001             
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ Using Device: {DEVICE}")

# ================= 2. æ ¸å¿ƒä¿®å¤ï¼šæ™ºèƒ½æ•°æ®åŠ è½½ =================
def smart_feature_slice(df_values):
    """
    ã€æ ¸å¿ƒä¿®å¤ã€‘å¼ºåˆ¶ç‰¹å¾å¯¹é½åˆ° 115 ç»´
    è§£å†³å› ç´¢å¼•åˆ—å¯¼è‡´çš„ç‰¹å¾å·¦ç§»å’Œå‡†ç¡®ç‡å´©ç›˜é—®é¢˜ã€‚
    """
    TARGET_DIM = 115
    current_dim = df_values.shape[1]
    
    if current_dim == TARGET_DIM:
        return df_values
    elif current_dim > TARGET_DIM:
        # ä¸¢å¼ƒå‰é¢çš„ç´¢å¼•åˆ—ï¼Œåªå–æœ€å115åˆ—
        return df_values[:, -TARGET_DIM:]
    else:
        # é˜²å¾¡æ€§è¡¥é›¶
        pad = np.zeros((df_values.shape[0], TARGET_DIM - current_dim))
        return np.hstack([df_values, pad])

def load_data_final():
    print(f"\n[1/4] Loading Data...")
    
    # --- è·¯å¾„è°ƒè¯• (ä¿®å¤æ‰¾ä¸åˆ°æ–‡ä»¶çš„é—®é¢˜) ---
    print(f"    å½“å‰ä»£ç ç›®å½•: {CURRENT_DIR}")
    print(f"    æ­£åœ¨å¯»æ‰¾æ•°æ®: {os.path.abspath(PATH_FEAT)}")
    
    if not os.path.exists(PATH_FEAT):
        print(f"\nâŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ï¼")
        print(f"   è¯·æ£€æŸ¥ 'Dataset' æ–‡ä»¶å¤¹æ˜¯å¦åœ¨ä»£ç çš„ä¸Šçº§ç›®å½•ä¸­ã€‚")
        return None
    # -------------------------------------

    try:
        # åŠ è½½ç‰¹å¾
        df_feat = pd.read_csv(PATH_FEAT, header=None)
        raw_feat = df_feat.values
        
        # ã€å…³é”®ã€‘æ‰§è¡Œæ™ºèƒ½åˆ‡ç‰‡ï¼Œç¡®ä¿æ²¡æœ‰ç´¢å¼•åˆ—å¹²æ‰°
        features = smart_feature_slice(raw_feat)
        
        # åŠ è½½æ ‡ç­¾
        if os.path.exists(PATH_LBL):
            df_lbl = pd.read_csv(PATH_LBL, header=None, low_memory=False)
            labels = df_lbl.iloc[:, -1].values.flatten().astype(int)
        else:
            # å¦‚æœæ²¡æœ‰å•ç‹¬çš„æ ‡ç­¾æ–‡ä»¶ï¼Œå‡è®¾æœ€åä¸€åˆ—æ˜¯æ ‡ç­¾ï¼ˆå¾ˆå°‘è§ï¼Œé˜²ä¸‡ä¸€ï¼‰
            print("âš ï¸ Warning: Label file not found, using last column of dataset.")
            labels = raw_feat[:, -1].astype(int)

    except Exception as e:
        print(f"âŒ æ•°æ®è¯»å–å‡ºé”™: {e}")
        return None

    # å¯¹é½é•¿åº¦
    min_len = min(len(labels), len(features))
    labels = labels[:min_len]
    features = features[:min_len]

    # æ•°æ®é›†åˆ’åˆ†ï¼šè®­ç»ƒé›†åªç”¨æ­£å¸¸æµé‡(0)ï¼Œæµ‹è¯•é›†åŒ…å«æ­£å¸¸(0)å’Œæ”»å‡»(1)
    benign_idx = np.where(labels == 0)[0]
    attack_idx = np.where(labels == 1)[0]
    
    # 80% æ­£å¸¸æµé‡ç”¨äºè®­ç»ƒ
    train_idx, test_benign_idx = train_test_split(benign_idx, test_size=0.2, random_state=42)
    # æµ‹è¯•é›† = 20% æ­£å¸¸ + å…¨éƒ¨æ”»å‡»
    test_idx = np.concatenate([test_benign_idx, attack_idx])

    X_train = features[train_idx]
    X_test = features[test_idx]
    y_test = labels[test_idx]

    # é¢„å¤„ç†ï¼šLog + RobustScaler (æŠ—å¼‚å¸¸å€¼)
    X_train = np.log1p(np.abs(X_train))
    X_test = np.log1p(np.abs(X_test))

    scaler = RobustScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    
    # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æŸäº›æç«¯å€¼
    X_train = np.clip(X_train, -10, 10)
    X_test = np.clip(X_test, -10, 10)

    print(f"    Data Loaded! Train: {X_train.shape}, Test: {X_test.shape}")
    return X_train, X_test, y_test, 115 # å¼ºåˆ¶è¿”å›115ç»´

# ================= 3. TGP-SSID æ¨¡å‹ (æ ‡å‡†ç¨³å¥ç‰ˆ) =================
class TGP_SSID(nn.Module):
    def __init__(self, input_dim):
        super(TGP_SSID, self).__init__()
        # ç¼–ç å™¨
        self.enc = nn.Sequential(
            nn.Linear(input_dim, 256), nn.LayerNorm(256), nn.LeakyReLU(0.2),
            nn.Linear(256, 128), nn.LayerNorm(128), nn.LeakyReLU(0.2),
            nn.Linear(128, LATENT_DIM) # No activation at latent
        )
        # è§£ç å™¨
        self.dec = nn.Sequential(
            nn.Linear(LATENT_DIM, 128), nn.LayerNorm(128), nn.LeakyReLU(0.2),
            nn.Linear(128, 256), nn.LayerNorm(256), nn.LeakyReLU(0.2),
            nn.Linear(256, input_dim)
        )
        # å‡ ä½•åŸå‹
        self.prototypes = nn.Parameter(torch.randn(NUM_PROTOTYPES, LATENT_DIM))

    def init_prototypes(self, x_train):
        print("    -> Initializing Prototypes...")
        self.eval()
        with torch.no_grad():
            # éšæœºé‡‡æ ·åˆå§‹åŒ–
            idx = np.random.choice(len(x_train), min(10000, len(x_train)), replace=False)
            x_sample = torch.FloatTensor(x_train[idx]).to(DEVICE)
            z = self.enc(x_sample).cpu().numpy()
        
        kmeans = KMeans(n_clusters=NUM_PROTOTYPES, n_init=10).fit(z)
        self.prototypes.data = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(DEVICE)
        self.train()

    def forward(self, x):
        z = self.enc(x)
        rec = self.dec(z)
        return rec, z

    def compute_score(self, x):
        self.eval()
        with torch.no_grad():
            x = x.to(DEVICE)
            rec, z = self.forward(x)
            
            # 1. é‡æ„è¯¯å·® (MSE)
            rec_err = torch.mean((x - rec) ** 2, dim=1)
            
            # 2. åŸå‹è·ç¦» (Euclidean)
            z_exp = z.unsqueeze(1)
            p_exp = self.prototypes.unsqueeze(0)
            # è®¡ç®—åˆ°æœ€è¿‘åŸå‹çš„è·ç¦»
            dists = torch.norm(z_exp - p_exp, dim=2)
            min_dist, _ = torch.min(dists, dim=1)
            
            # 3. ç®€å•åŠ æƒèåˆ (ä¸åšå¤æ‚çš„ Log å˜æ¢ï¼Œä¿æŒçº¿æ€§å¯åˆ†)
            score = rec_err + min_dist
            
        return score.cpu().numpy()

# ================= 4. ä¸»ç¨‹åº =================
if __name__ == "__main__":
    # 1. åŠ è½½
    data = load_data_final()
    if data is None: 
        print("âŒ ç¨‹åºå› æ•°æ®åŠ è½½å¤±è´¥è€Œç»ˆæ­¢ã€‚")
        exit()
        
    X_train, X_test, y_test, input_dim = data

    # 2. è®­ç»ƒ
    print(f"\n[2/4] Training TGP-SSID...")
    train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_train))
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    model = TGP_SSID(input_dim).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR)

    # åˆå§‹åŒ–åŸå‹
    model.init_prototypes(X_train)

    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        for batch in train_loader:
            x = batch[0].to(DEVICE)
            optimizer.zero_grad()
            rec, z = model(x)

            # æŸå¤± = é‡æ„æŸå¤± + åŸå‹è·ç¦»æŸå¤±
            rec_loss = F.mse_loss(rec, x)
            
            z_exp = z.unsqueeze(1)
            p_exp = model.prototypes.unsqueeze(0)
            dists = torch.norm(z_exp - p_exp, dim=2)
            min_dist, _ = torch.min(dists, dim=1)
            proto_loss = torch.mean(min_dist)

            loss = rec_loss + 0.1 * proto_loss # ç®€å•æœ‰æ•ˆçš„æŸå¤±ç»„åˆ

            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        if (epoch + 1) % 10 == 0:
            print(f"    Epoch {epoch+1}/{EPOCHS}: Loss = {total_loss/len(train_loader):.6f}")

    # 3. è¯„ä¼°
    print(f"\n[3/4] Evaluating...")
    scores = []
    # åˆ†æ‰¹æ¬¡è¯„ä¼°é˜²æ­¢æ˜¾å­˜çˆ†ç‚¸
    batch_size_eval = 5000
    for i in range(0, len(X_test), batch_size_eval):
        bx = torch.FloatTensor(X_test[i:i+batch_size_eval])
        scores.append(model.compute_score(bx))
    scores = np.concatenate(scores)

    # 4. ç»“æœä¸ç»˜å›¾
    auc_val = roc_auc_score(y_test, scores)
    fpr, tpr, thresholds = roc_curve(y_test, scores)
    
    # å¯»æ‰¾æœ€ä½³é˜ˆå€¼ (Youden's J statistic)
    optimal_idx = np.argmax(tpr - fpr)
    threshold = thresholds[optimal_idx]

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    y_pred = (scores > threshold).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    
    tpr_val = tp / (tp + fn)
    tnr_val = tn / (tn + fp)
    acc_val = (tp + tn) / len(y_test)

    print("\n" + "="*60)
    print("ğŸ† === Experiment 2 Results (Pro) ===")
    print("="*60)
    print(f"    ACC (Accuracy): {acc_val*100:.2f}%")
    print(f"    TPR (Recall):   {tpr_val*100:.2f}%")
    print(f"    TNR (Spec.):    {tnr_val*100:.2f}%")
    print(f"    AUC Score:      {auc_val:.4f}")
    print("="*60)

    # ç»˜å›¾
    print(f"\n[4/4] Generating Plots...")
    try:
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        # KDE
        ax1 = axes[0]
        sns.kdeplot(scores[y_test==0], fill=True, label='Normal', color='green', ax=ax1, warn_singular=False)
        if np.sum(y_test==1) > 0:
            sns.kdeplot(scores[y_test==1], fill=True, label='Unknown (Mirai)', color='red', ax=ax1, warn_singular=False)
        ax1.axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Thr={threshold:.3f}')
        # å¦‚æœåˆ†æ•°å·®å¼‚å¤ªå¤§ï¼Œç”¨ log scale æ˜¾ç¤ºä¼šæ›´å¥½çœ‹
        # ax1.set_xscale('log') 
        ax1.set_title(f'Score Distribution (AUC={auc_val:.4f})')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # ROC
        ax2 = axes[1]
        ax2.plot(fpr, tpr, color='blue', linewidth=2.5, label=f'TGP-SSID')
        ax2.plot([0, 1], [0, 1], color='gray', linestyle='--')
        ax2.scatter([fpr[optimal_idx]], [tpr[optimal_idx]], c='red', s=100, label=f'Optimal')
        ax2.set_title('ROC Curve')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        output_img = os.path.join(CURRENT_DIR, '../Figure/Exp2_Kitsune_Result_Pro.png')
        os.makedirs(os.path.dirname(output_img), exist_ok=True)
        plt.savefig(output_img, dpi=300)
        print(f"âœ… Plot saved to: {output_img}")
        # plt.show() # å¦‚éœ€å¼¹çª—æ˜¾ç¤ºè¯·å–æ¶ˆæ³¨é‡Š
        
    except Exception as e:
        print(f"âš ï¸ Plotting Error: {e}")